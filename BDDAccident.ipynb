{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "462ab954-f8c3-441b-bb37-890f9eb60b67",
      "metadata": {
        "id": "462ab954-f8c3-441b-bb37-890f9eb60b67"
      },
      "source": [
        "##DEVOIR 1\n",
        "\n",
        "* Hélène HU 22009784\n",
        "* Nicolas PENELOUX 22001302\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PREAMBULE\n",
        "\n",
        "Voici notre rendu concernant l'analyse de la base de données annuelles des accidents corporels de la circulation routière, fait par Hélène HU et Nicolas PENELOUX. Il y aura deux rendus, un avec compilation, et un sans, dans le cas ou vous auriez des problèmes à compiler le code du fichier : nous avions nous même eu quelques problèmes à ce sujet, notamment pour pyspark que nous utilisons ; nous voulons vous montrez que nous avons bien travailler le devoir.\n",
        "\n",
        "Pour le gestionnaire de données, nous avons utilisé pyspark. Notre choix était motivé parce qu'il nous semblait plus pratique de l'utiliser que pandas, même si dans certains cas il semblerait que pandas soit plus efficace, notamment pour l'utilisation de plotly : parfois, plotly ne fonctionnait pas avec des dataframes spark, et dans ce cas nous utilisons les dataframes pandas, en transformant les dfs spark avec \"**toPandas()**\".\n",
        "\n",
        "Egalement, nous avons privilégié plotly à altair, car les objets graphiques produit par plotly nous semblait plus agréable à regarder que ceux d'altair.\n",
        "\n",
        "Enfin, nous avons laissé les lignes de commandes pour installer pyspark, ainsi qu'une autre pour installer une extension particulière de plotly (que nous utilisons à la question 5), dans le cas où vous ne l'aviez pas. En espérant que cela ne gâche pas votre compilation.\n"
      ],
      "metadata": {
        "id": "3gBvTzJx0s18"
      },
      "id": "3gBvTzJx0s18"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install plotly-calplot"
      ],
      "metadata": {
        "id": "A3yE8Gv-l8Fy"
      },
      "id": "A3yE8Gv-l8Fy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chargement des fichiers pour les années 2021, 2022\n",
        "\n",
        "Pour le chargement des fichiers, on commence d'abord par créé le sous répertoire data pour stocker nos fichiers csv. Ensuite, on stocke dans une liste, les urls de téléchargements des fichiers souhaités : On stocke à la même occasion les noms de chaque fichier, pour pouvoir les reconnaître plus tard.\n",
        "\n",
        "En utilisant le module \"*request*\", on récupère chaque fichier à l'aide de la fonction \"*get(url)*\". Enfin on écris dans le sous répertoire data le contenu de la requête."
      ],
      "metadata": {
        "id": "GxjXPr903yzt"
      },
      "id": "GxjXPr903yzt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ee63e8-8b85-40ba-bdb4-6fa5cd899526",
      "metadata": {
        "id": "68ee63e8-8b85-40ba-bdb4-6fa5cd899526"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Créer le répertoire \"data\" s'il n'existe pas\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.makedirs(\"data\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Liste des URLs des fichiers à télécharger pour les années 2021 et 2022\n",
        "files = [\n",
        "    (\"https://www.data.gouv.fr/fr/datasets/r/62c20524-d442-46f5-bfd8-982c59763ec8\", \"usagers-2022\"),\n",
        "    (\"https://www.data.gouv.fr/fr/datasets/r/c9742921-4427-41e5-81bc-f13af8bc31a0\", \"vehicules-2022\"),\n",
        "    (\"https://www.data.gouv.fr/fr/datasets/r/a6ef711a-1f03-44cb-921a-0ce8ec975995\",\"lieux-2022\"),\n",
        "    (\"https://www.data.gouv.fr/fr/datasets/r/5fc299c0-4598-4c29-b74c-6a67b0cc27e7\",\"caracteristiques-2022\"),\n",
        "    (\"https://www.data.gouv.fr/fr/datasets/r/ba5a1956-7e82-41b7-a602-89d7dd484d7a\",\"usagers-2021\"),\n",
        "    (\"https://www.data.gouv.fr/fr/datasets/r/0bb5953a-25d8-46f8-8c25-b5c2f5ba905e\",\"vehicules-2021\"),\n",
        "    (\"https://www.data.gouv.fr/fr/datasets/r/8a4935aa-38cd-43af-bf10-0209d6d17434\",\"lieux-2021\"),\n",
        "    (\"https://www.data.gouv.fr/fr/datasets/r/85cfdc0c-23e4-4674-9bcd-79a970d7269b\",\"caracteristiques-2021\"),\n",
        "]\n",
        "\n",
        "# Début des téléchargements\n",
        "for url, name in files:\n",
        "    response = requests.get(url)\n",
        "    # On vérifie que le téléchargement s'est bien passé\n",
        "    if response.status_code == 200:\n",
        "        # On écris le contenu du fichier dans le sous répertoire \"data\"\n",
        "        with open(\"data/\" + name + \".csv\", \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "            print(\"Téléchargement du fichier \" + name + \" terminé.\")\n",
        "    else:\n",
        "      # Problème de téléchargement d'un fichier\n",
        "        print(f\"Erreur lors du téléchargement du fichier \" + name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Création de dataframes correspondants au fichiers des années 2021, 2022 et nettoyage des données\n",
        "\n",
        "Partie du code un peu plus intéressante que la précédente. Tout d'abord, on commence bien sûr par créer notre session Spark. On rajoute à cette session la config \"*config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")*\" : nous avons eu des problèmes de versions de spark quand il fallait traiter les données de type *TimeStamp* (problème de Parser pour être plus précis), donc nous avons trouver le changement de config comme solution.\n",
        "\n",
        "Donc, pour traiter les données, nous avons fait une fonction **chargement_et_nettoyage_DF** qui prend en paramètre le nom du fichier qui servira de dataframe. Pour chaque fichier du sous répertoire, on appelle cette fonction.\n",
        "\n",
        "Dans cette fonction, on commence d'abord par créer le dataframe spark. Ensuite, pour chaque colonne du dataframe, on convertis les valeurs \"non renseignées\" (équivalente à -1 la plupart du temps) en valeur NULL, ce qui correspond pour nous, d'avantage à ce que devrais être une valeur non renseignée. Il faut faire attention, car le type *Timestamp* pose des problèmes : on ne peut pas simplement vérifié si la valeur de la colonne de type *TimeStamp* soit égale à -1, spark ne l'autorise pas, donc on traite toute les colonnes sauf celle qui sont des *TimeStamp*.\n",
        "  \n",
        "Ensuite, on regarde pour chaque fichier, si son nom contient \"*usagers*\", \"*caracteristiques*\", \"*lieux*\" ou \"*vehicules*\". Cela permet de traiter au cas par cas le dataframe en fonction de ce qu'il est réellement, et cela permet de respecter le principe de DRY (faire une fonction qui permet de traiter plusieurs fichiers différents en même temps, plutôt que recopier le code), et également la possibilité de gérer les données sur les fichiers de la BDD des accidents d'une autre année.\n",
        "\n",
        "Si nous avons un fichier \"*usagers*\", nous procédons le nettoyage ainsi :\n",
        "\n",
        "- On transforme les identifiants \"usager\" et \"vehicule\" en Integer. Ils sont à la base stocké sous format String, ce qui est illogique pour un identifiant d'une table. On utilise une fonction UDF préalablement créer \"**convert_id_to_int**\" qui prend en argument une chaîne de caractère et renvoie cette chaîne sous format Int.\n",
        "- On indexe la colonne *num_veh*, qui est une colonne catégorielle mais en format String.\n",
        "- On retire les colonnes \"*secu3*\" et \"*etatp*\", qui contiennent trop de valeur NULL.\n",
        "\n",
        "Pour un fichier \"*lieux*\" :\n",
        "\n",
        "- Les colonnes \"pr\" et \"pr1\" sont de type String alors qu'ils devraient être des Integer. De plus, les valeurs \"non renseignée\" sont censé être des \"-1\", alors qu'ici ils sont des \"(1)\". On remplace ça par NULL, puis on convertis le type en Integer.\n",
        "- Certaines colonne String contiennent des valeurs \"N/A\" quand les valeurs ne sont pas renseignée, on remplace par NULL.\n",
        "- On supprime les colonnes \"*lartpc*\" et \"*larrout*\", trop de NULL.\n",
        "\n",
        "Pour un fichier \"*caracteristiques*\" :\n",
        "\n",
        "- Dans un des fichiers, la colonne \"*Num_Acc*\" est remplacé par \"*Accident_Id*\", on remplace le nom.\n",
        "- Les latitudes et longitudes sont en String à cause de la virgule qui sépare la partie entière de la partie décimale, on remplace la virgule par un point puis on met en type Float.\n",
        "- On supprime les colonnes \"an\" \"jour\" \"mois\" et \"hmrn\", pour faire une grosse colonne de type *TimeStamp* de format \"yyyy-MM-dd HH:mm\"\n",
        "\n",
        "Enfin pour un fichier \"*vehicules*\" :\n",
        "\n",
        "- Comme pour \"*usagers*\", on s'occupe de l'identifiant \"vehicule\" de la même façon. On indexe également la colonne *num_veh*.\n",
        "- On retire la colonne occutc, qui a trop de valeur NULL.\n",
        "\n",
        "Finalement, après le traitement, la fonction renvoie le dataframe créé. Ensuite, on rajoute à la liste des dataframes celui qu'on vient de créer. On en profitera pour stocker dans une liste les noms de chaque dataframe, cela nous servira pour le merging des dataframes."
      ],
      "metadata": {
        "id": "Q0DbxPHE3x3G"
      },
      "id": "Q0DbxPHE3x3G"
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark as ps\n",
        "import re\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import *\n",
        "import plotly.express as px\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "#Fonction UDF pour convertir les id String en entier, on vire tout les caractères non numériques\n",
        "def convert_id_to_int(id):\n",
        "    id_without_spaces = re.sub(r'\\D', '', id)\n",
        "    return int(id_without_spaces)\n",
        "\n",
        "# Fonction pour respecter le DRY\n",
        "def chargement_et_nettoyage_DF(name) :\n",
        "  convert_id_to_int_udf = F.udf(convert_id_to_int, IntegerType())\n",
        "  df = spark.read.csv(name,header=True, sep=\";\", inferSchema=True)\n",
        "\n",
        "  for column in df.columns:\n",
        "    #Timestamp est incompatible avec le type Int pour -1, donc on vérifie si la colonne est pas du type Timestamp\n",
        "    #Par ailleurs, si on met deux fois la même ligne pour transformer les -1 en NULL, c'est parce que étrangement\n",
        "    #  avec \"-1\", il y a des colonnes qui ne sont pas affectés par la transformation de -1 à NULL.\n",
        "      if not isinstance(df.schema[column].dataType, TimestampType):\n",
        "        df = df.withColumn(column, when(col(column) == \"-1\", lit(None)).otherwise(col(column)))\n",
        "        df = df.withColumn(column, when(col(column) == -1, lit(None)).otherwise(col(column)))\n",
        "        pass\n",
        "\n",
        "  # Les usagers\n",
        "  if re.search(\"usagers\",name):\n",
        "    #On transforme id_usager et id_vehicule en colonne Int, en retirant au préalable les \" \" que contiennent les\n",
        "    #valeurs.\n",
        "    df = df.withColumn(\"id_usager_int\", convert_id_to_int_udf(\"id_usager\"))\n",
        "    df = df.withColumn(\"id_vehicule_int\", convert_id_to_int_udf(\"id_vehicule\"))\n",
        "    df = df.drop(\"id_usager\")\n",
        "    df = df.drop(\"id_vehicule\")\n",
        "    df = df.withColumnRenamed(\"id_usager_int\",\"id_usager\")\n",
        "    df = df.withColumnRenamed(\"id_vehicule_int\",\"id_vehicule\")\n",
        "\n",
        "\n",
        "    #Indexation de num_veh\n",
        "    indexer = StringIndexer(inputCol='num_veh',outputCol='num_veh_idx')\n",
        "    indexer = indexer.fit(df)\n",
        "    df = indexer.transform(df)\n",
        "\n",
        "    colonne = [\"Num_Acc\",\"id_usager\",\"id_vehicule\",\"num_veh\",\"place\",\"catu\",\"grav\",\"sexe\",\"an_nais\",\"trajet\",\"secu1\",\"secu2\",\"secu3\",\"locp\",\"actp\",\"etatp\"]\n",
        "    df = df.select(colonne)\n",
        "\n",
        "    #On retire les colonnes secu3 et etatp qui sont quasi inutile\n",
        "    df = df.drop(\"secu3\")\n",
        "    df = df.drop(\"etatp\")\n",
        "    df.show()\n",
        "\n",
        "  # Les lieux\n",
        "  elif re.search(\"lieux\",name):\n",
        "    #Nettoyage des données : on transforme les (1) en None et les -1 en None\n",
        "    df = df.withColumn(\"pr\", when(col(\"pr\") == \"(1)\", lit(None)).otherwise(col(\"pr\")))\n",
        "    df = df.withColumn(\"pr1\", when(col(\"pr1\") == \"(1)\", lit(None)).otherwise(col(\"pr1\")))\n",
        "\n",
        "\n",
        "    for column in df.columns:\n",
        "      #Il y avait des colonnes String avec des valeurs \"N/A\", on les remplace par NULL\n",
        "        df = df.withColumn(column, when(col(column) == \"N/A\", lit(None)).otherwise(col(column)))\n",
        "\n",
        "    #Changement de typage de String à Int pour pr, pr1\n",
        "    df = df.withColumn(\"pr\", df.pr.cast(IntegerType()))\n",
        "    df = df.withColumn(\"pr1\", df.pr1.cast(IntegerType()))\n",
        "\n",
        "    #On drop les colonnesa vec trop de NULL\n",
        "    df = df.drop(\"lartpc\")\n",
        "    df = df.drop(\"larrout\")\n",
        "\n",
        "    df.show()\n",
        "  # Les caractéristiques\n",
        "  elif re.search(\"caracteristiques\",name):\n",
        "    # On renome Accident_Id en Num_Acc\n",
        "    if \"Accident_Id\" in df.columns:\n",
        "      df = df.withColumnRenamed(\"Accident_Id\",\"Num_Acc\")\n",
        "\n",
        "    # On met la latitude et la longitude en valeur numérique (dans notre cas, en Float)\n",
        "    df = df.withColumn(\"lat\", regexp_replace(col(\"lat\"), \",\", \".\"))\n",
        "    df = df.withColumn(\"long\", regexp_replace(col(\"long\"), \",\", \".\"))\n",
        "    df = df.withColumn(\"lat\", df.lat.cast(FloatType()))\n",
        "    df = df.withColumn(\"long\", df.long.cast(FloatType()))\n",
        "\n",
        "    # Ici, on forme une unique colonne \"datetime\" de type TimeStamp, on supprime \"hrmn\", \"an\" \"jour\" et \"mois\"\n",
        "    df = df.withColumn(\"heure\", hour(\"hrmn\"))\n",
        "    df = df.withColumn(\"minute\", minute(\"hrmn\"))\n",
        "\n",
        "    df = df.withColumn(\"heure_minute\", concat(col(\"heure\"), lit(\":\"), col(\"minute\")))\n",
        "    df = df.withColumn(\"datetime\", to_timestamp(concat(col(\"an\"), lit(\"-\"), col(\"mois\"), lit(\"-\"), col(\"jour\"), lit(\" \"), col(\"heure_minute\")), \"yyyy-MM-dd HH:mm\"))\n",
        "    df = df.drop(\"heure\")\n",
        "    df = df.drop(\"minute\")\n",
        "    df = df.drop(\"heure_minute\")\n",
        "    df = df.drop(\"an\")\n",
        "    df = df.drop(\"mois\")\n",
        "    df = df.drop(\"jour\")\n",
        "    df = df.drop(\"hrmn\")\n",
        "\n",
        "    #On range dans l'ordre\n",
        "    colonnes = [\"Num_Acc\",\"datetime\",\"lum\",\"dep\",\"com\",\"agg\",\"int\",\"atm\",\"col\",\"adr\",\"lat\",\"long\"]\n",
        "    df = df.select(colonnes)\n",
        "    df.show()\n",
        "\n",
        "  # Les véhicules\n",
        "  elif re.search(\"vehicules\",name):\n",
        "    #On transforme les id_vehicule de String en Int\n",
        "    df = df.withColumn(\"id_vehicule_int\", convert_id_to_int_udf(\"id_vehicule\"))\n",
        "    df = df.drop(\"id_vehicule\")\n",
        "    df = df.withColumnRenamed(\"id_vehicule_int\",\"id_vehicule\")\n",
        "\n",
        "    #Indexation de num_veh\n",
        "    indexer = StringIndexer(inputCol='num_veh',outputCol='num_veh_idx')\n",
        "    indexer = indexer.fit(df)\n",
        "    df = indexer.transform(df)\n",
        "\n",
        "    #Suppression de occutc, qui est quasi vide\n",
        "    df = df.drop(\"occutc\")\n",
        "\n",
        "    #On remet les colonnes dans l'ordre\n",
        "    colonnes = [\"Num_Acc\",\"id_vehicule\",\"num_veh\",\"senc\",\"catv\",\"obs\",\"obsm\",\"choc\",\"manv\",\"motor\"]\n",
        "    df = df.select(colonnes)\n",
        "    df.show()\n",
        "\n",
        "  else :\n",
        "    print(\"Fichier invalide pour le nettoyage.\")\n",
        "    exit(1)\n",
        "\n",
        "\n",
        "  return df\n",
        "\n",
        "spark = SparkSession.builder.appName(\"BDDAccident\").config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\").getOrCreate()\n",
        "# Problème de version de spark, on utilise ceci pour utiliser le comportement du parser de date et heure d'avant spark 3.0\n",
        "\n",
        "\n",
        "\n",
        "dataframes = []\n",
        "names = []\n",
        "for url,name in files:\n",
        "  df = chargement_et_nettoyage_DF(\"data/\" + name + \".csv\")\n",
        "  dataframes.append(df)\n",
        "  names.append(name)\n"
      ],
      "metadata": {
        "id": "b1d31FyWbpnO"
      },
      "id": "b1d31FyWbpnO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Réunion des années 2021, 2022\n",
        "\n",
        "Pour respecter les principes de DRY, on créer une fonction **merge_dfs**, qui s'occupera de fusionner les dataframes similaire d'une année à l'autre.\n",
        "\n",
        "La fonction prend en paramètre la liste des dataframes, et la liste de noms récupéré juste avant.\n",
        "\n",
        "L'algorithme fonctionne comme ceci :\n",
        "\n",
        "1. On initialise un dictionnaire vide grouped_dfs.\n",
        "2. On parcourt simultanément la liste des dataframes et des noms. On récupère le nom, on sépare le nom de l'année (\"usagers-2022\" -> \"usagers\" ; \"2022\"). Ensuite, on regarde si le nom n'est pas déjà dans le dictionnaire grouped_dfs, si c'est cas alors on ajoute une entrée pour ce nom de groupe avec une liste vide. Sinon, il ajoute le Dataframe actuel à la liste correspondant au nom du groupe dans le dictionnaire.\n",
        "3. On initialise une liste vide \"merged_dfs\" pour stocker les futurs réunions.\n",
        "4. On parcourt \"grouped_dfs\". Pour chaque nom de groupe, on fusionne tous les dataframes de la liste en un seul dataframe, puis on l'ajoute à \"merged_dfs\"\n",
        "5. On retourne la liste \"merged_dfs\", désormais remplis.\n",
        "\n",
        "On utilise l'algorithme sur chacun de nos dataframes."
      ],
      "metadata": {
        "id": "MXW4T-U-35hN"
      },
      "id": "MXW4T-U-35hN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f70186-9d1a-4062-a143-25d238f553f8",
      "metadata": {
        "id": "d2f70186-9d1a-4062-a143-25d238f553f8"
      },
      "outputs": [],
      "source": [
        "# Fonction de merge\n",
        "def merge_dfs(dfs,names):\n",
        "  grouped_dfs = {}\n",
        "  # On parcours simultanément les dataframes et les noms\n",
        "  for df, nom_df in zip(dfs,names):\n",
        "    # On sépare le nom en deux \"nom\" \"annee\"\n",
        "        name = nom_df.split(\"-\")[0]\n",
        "        # Si le nom du df n'est pas dans notre dictionnaire, on ajoute une entrée avec le nom actuel\n",
        "        if name not in grouped_dfs:\n",
        "            grouped_dfs[name] = []\n",
        "            # sinon, on ajoute le df actuel à celui qui existe déjà\n",
        "        grouped_dfs[name].append(df)\n",
        "\n",
        "  merged_dfs = []\n",
        "  # On parcours chaque élément de notre dictionnaire, et on fusionne tout les dataframes équivalent\n",
        "  for name, dfs in grouped_dfs.items():\n",
        "        merged_df = dfs[0]\n",
        "        for df in dfs[1:]:\n",
        "            merged_df = merged_df.union(df)\n",
        "        merged_dfs.append(merged_df)\n",
        "\n",
        "  return merged_dfs\n",
        "\n",
        "\n",
        "merged_dfs = merge_dfs(dataframes,names)\n",
        "\n",
        "for df in merged_dfs:\n",
        "  df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Résumés numériques\n",
        "\n",
        "Pour cette question, il fallait donc faire le résumé des valeurs numériques de nos dataframes. Seulement, dans la base de donnée, il y a beaucoup de valeurs numériques (*Int, double, float*) mais la plupart sont des colonne catégorielles.\n",
        "De ce fait, il est impossible de calculer la moyenne, la médiane ... de valeurs catégorielles, cela n'a pas de sens.\n",
        "\n",
        "Dans ce cas, nous avons décidé de faire les résumés des années de naissances, de la vitesse maximale autorisé, et l'heure des accidents. Comme autres résumés numériques possible, nous aurions pu prendre le nombre d'occupants dans le transport en commun (\"*occutc*\"), la largeur du terre plein central (\"*lartpc*\") et la largeur de la chaussée affectée à la circulation des véhicules(\"*larrout*\"), seulement déjà cela nous semblait pas forcément intéressant, et surtout ces trois colonnes ont été nettoyés avant, car il y avait trop de valeur NULL.    \n",
        "    \n",
        "Nous calculons \"manuellement\" le résumé numérique. Il existe une fonction \"summary\" sur pyspark, mais elle ne permet pas de calculer dirrectement l'asymétrie et l'applatissement, donc on a décidé de tout calculer par nous même, toujours en utilisant le module spark. Cependant, nous avons utilisé **summary()** pour le résumé des heures, car comme à la question du nettoyage des données, il semblerait y avoir un problème au niveau de la version spark, ce qui empêche le traitement correct des valeurs de type TimeStamp. On ne pouvait pas calculer la médiane ni les quartiles, par contre **summary()** lui le fait très bien.\n",
        "\n",
        "Enfin, pour afficher les boxplots des valeurs numériques, on utilise le module \"pandas\", car il semblerait que plotly ne puisse pas utilisé \"pyspark\", ou du moins cela semble compliqué. Dans ce cas, on utilise la fonction \"**toPandas()**\" qui transforme notre dataframe en dataframe pandas. On peut désormais l'utiliser dans notre boxplot plotly."
      ],
      "metadata": {
        "id": "qUuMmBV73-AG"
      },
      "id": "qUuMmBV73-AG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb5bdd8-0ee4-46fb-9f7d-680284634004",
      "metadata": {
        "id": "8cb5bdd8-0ee4-46fb-9f7d-680284634004"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "#Nos différents dataframes\n",
        "df_users = merged_dfs[0]\n",
        "df_vehicules = merged_dfs[1]\n",
        "df_locations = merged_dfs[2]\n",
        "df_caracteristiques = merged_dfs[3]\n",
        "\n",
        "df_users_an_naiss = df_users.select(\"an_nais\") # les années de naissances\n",
        "df_caracteristiques = df_caracteristiques.withColumn(\"heure\", hour(\"datetime\"))\n",
        "temps_total = df_caracteristiques.select(\"heure\") #Les heures des accidents\n",
        "vma_max = df_locations.select(\"vma\") #La vitesse max\n",
        "\n",
        "\n",
        "\n",
        "def resume_et_boxplot(df,colonne,title):\n",
        "   # Extraction de la colonne spécifiée\n",
        "    df = df.withColumn(colonne, col(colonne))\n",
        "\n",
        "    # Résumé numérique\n",
        "    # Calcul des statistiques\n",
        "    if colonne != \"heure\":\n",
        "      statistics = {\n",
        "          \"mean\": df.agg({colonne: \"mean\"}).collect()[0][0],\n",
        "          \"median\": df.approxQuantile(colonne, [0.5], 0.001)[0],\n",
        "          \"quartiles\": df.approxQuantile(colonne, [0.25, 0.75], 0.001),\n",
        "          \"stddev\": df.agg({colonne: \"stddev\"}).collect()[0][0],\n",
        "          \"skewness\": df.agg({colonne: \"skewness\"}).collect()[0][0],\n",
        "          \"kurtosis\": df.agg({colonne: \"kurtosis\"}).collect()[0][0]\n",
        "      }\n",
        "\n",
        "      # Affichage des statistiques\n",
        "      print(f\"Statistiques pour la colonne '{colonne}':\")\n",
        "      print(f\"Moyenne: {statistics['mean']}\")\n",
        "      print(f\"Médiane: {statistics['median']}\")\n",
        "      print(f\"1er Quartile: {statistics['quartiles'][0]}\")\n",
        "      print(f\"3ème Quartile: {statistics['quartiles'][1]}\")\n",
        "      print(f\"Écart-type: {statistics['stddev']}\")\n",
        "      print(f\"Asymétrie: {statistics['skewness']}\")\n",
        "      print(f\"Applatissement: {statistics['kurtosis']}\")\n",
        "      print(\"\")\n",
        "    else :\n",
        "\n",
        "      #Cas particulier pour la lecture des heures : on ne pouvait pas calculer la médiane et les quartiles\n",
        "      # pour les heures, malgré que ce soit en type Integer.\n",
        "      asymetrie = df.agg({colonne: \"skewness\"}).collect()[0][0]\n",
        "      stddev = df.agg({colonne: \"stddev\"}).collect()[0][0]\n",
        "      kurtosis = df.agg({colonne : \"kurtosis\"}).collect()[0][0]\n",
        "      summary = df.select(colonne).summary()\n",
        "      summary.show()\n",
        "      print(f\"Écart-type: {stddev}\")\n",
        "      print(f\"Asymétrie: {asymetrie}\")\n",
        "      print(f\"Applatissement: {kurtosis}\")\n",
        "\n",
        "\n",
        "    # Génération du boxplot\n",
        "    fig = px.box(df.toPandas(), y=colonne, title=title)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "resume_et_boxplot(df_users_an_naiss,\"an_nais\",\"Boxplot des années de naissance\")\n",
        "resume_et_boxplot(temps_total,\"heure\",\"Boxplot des heures des accidents\")\n",
        "resume_et_boxplot(vma_max,\"vma\",\"Boxplot de la vitesse maximale autorisée sur le lieu de l'accident\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Répartition\n",
        "\n",
        "  Pour les deux répartitions qui suivent, on va créer une fonction pour chaque, qui prendra en argument un dataframe, car on les réutiliseras pour les accidents impliquant des cyclistes et des piétons.\n",
        "\n",
        "*  ### Répartition des accidents sur la semaine (jours et heures)\n",
        "\n",
        "  Pour les accidents sur la semaine, on récupère sur *datetime* (date et heure de l'accident) les jours de la semaine via la fonction \"**dayofweek**\" de spark, et les heures avec \"**hour**\", et on compte le nombre d'accident par jour et par heure de la semaine.  \n",
        "  On n'oublie pas de remplacer les valeurs numériques des jours par ceux qu'ils représentent, en faisant un \"**map**\" sur nos jours de la semaines.\n",
        "  On affiche ça dans un graphique en barre, avec en ordonnée le nombre d'accidents et en abscisse les heures. En couleur, on met les jours de la semaine qui sont associés aux accidents par tranche horaire.\n",
        "\n",
        "\n",
        "* ### Répartition des accidents sur les mois de l’année\n",
        "\n",
        "  Pour les accidents sur les mois de l'année, on récupère sur *datetime* les mois avec la fonction **month**, puis de même, on compte le nombre d'accidents lié à chaque mois. On remplace également les numéros des mois par leurs valeurs respective.\n",
        "  On affiche également dans un graphe en barre, en abscisse les mois, et en ordonnée le nombre d'accident.\n",
        "\n",
        "* ### Bonus : Accident par jour pendant les années 2021 et 2022\n",
        "\n",
        "  En petit bonus, en utilisant l'extension \"*calplot*\" de plotly, on peut afficher dans un calendrier heatmap le nombres d'accidents par jour pendant les années 2021 et 2022. Pour cela, on compte le nombre d'accident pour chaque jour, on créer notre intervalle de date pour calplot (on met de force les dates \"2021-01-01\" et \"2022-12-31\", mais il serait plus judicieux de prendre d'abord la date la plus ancienne et la plus récente en guise d'intervalle), avec en abscisse les jours de la semaine, et en ordonnée les mois.\n"
      ],
      "metadata": {
        "id": "Q_QnpUhO4EYS"
      },
      "id": "Q_QnpUhO4EYS"
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly_calplot import calplot\n",
        "\n",
        "caracteristiques = merged_dfs[3]\n",
        "# Map pour les jour de la semaines et les mois\n",
        "jour_semaine_map = {1: 'Lundi', 2: 'Mardi', 3: 'Mercredi', 4: 'Jeudi', 5: 'Vendredi', 6: 'Samedi', 7: 'Dimanche'}\n",
        "mois_map = {1:'Janvier', 2:'Fevrier', 3:'Mars', 4:'Avril', 5:'Mai', 6:'Juin', 7:\"Juillet\", 8:'Aout', 9:'Septembre', 10:'Octobre', 11:'Novembre', 12:'Decembre'}\n",
        "\n",
        "# Fonction qui affiche les accidents par jour de la semaine et heures\n",
        "def show_accidents_par_jour_et_heure (df_cara):\n",
        "  # Requête PySpark pour compter le nombre d'accidents par jour de la semaine et par heure\n",
        "  accidents_par_jour_et_heure = df_cara.groupBy(hour(\"datetime\").alias(\"heure\"), dayofweek(\"datetime\").alias(\"jour_semaine\")) \\\n",
        "                                              .count() \\\n",
        "                                              .orderBy(\"jour_semaine\", \"heure\")\n",
        "\n",
        "  # Conversion du DataFrame Spark en DataFrame Pandas\n",
        "  df_pandas = accidents_par_jour_et_heure.toPandas()\n",
        "  df_pandas[\"jour_semaine\"] = df_pandas[\"jour_semaine\"].map(jour_semaine_map)\n",
        "\n",
        "\n",
        "  # Graphique en barre\n",
        "  fig = px.bar(df_pandas, y=\"count\", x=\"heure\", color=\"jour_semaine\",\n",
        "             labels={\"heure\": \"Heure\", \"count\": \"Nombre d'accidents\", \"jour_semaine\": \"Jour de la semaine\"},\n",
        "             title=\"Répartition des accidents sur la semaine (jours et heures)\")\n",
        "  fig.update_layout(barmode='stack', xaxis={'categoryorder':'total ascending'})\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "show_accidents_par_jour_et_heure(caracteristiques)\n",
        "\n",
        "\n",
        "# Fonction qui affiche les accidents par mois\n",
        "def show_accidents_par_mois (df_cara):\n",
        "  # Groupement des données par mois\n",
        "  accidents_par_mois = df_cara.groupBy(month(\"datetime\").alias(\"mois\")) \\\n",
        "                                     .count() \\\n",
        "                                     .orderBy(\"mois\")\n",
        "\n",
        "  df_pandas = accidents_par_mois.toPandas()\n",
        "  df_pandas[\"mois\"] = df_pandas[\"mois\"].map(mois_map)\n",
        "\n",
        "\n",
        "  # Graphe en barre\n",
        "  fig_histogramme = px.bar(df_pandas,\n",
        "                         x=\"mois\",\n",
        "                         y=\"count\",\n",
        "                         labels={\"mois\": \"Mois\", \"count\": \"Nombre d'accidents\"},\n",
        "                         title=\"Répartition des accidents sur les mois de l'année\")\n",
        "  fig_histogramme.show()\n",
        "\n",
        "show_accidents_par_mois(caracteristiques)\n",
        "\n",
        "\n",
        "\n",
        "accidents_par_jour = caracteristiques.groupBy(date_format(\"datetime\", \"yyyy-MM-dd\").alias(\"jour\")) \\\n",
        "                                      .count() \\\n",
        "                                      .orderBy(\"jour\")\n",
        "# Création de la heatmap\n",
        "df_pandas = accidents_par_jour.toPandas()\n",
        "num_rows = len(df_pandas)\n",
        "\n",
        "# Intervalle de date de notre BDD\n",
        "start_date = '2021-01-01'\n",
        "end_date = '2022-12-31'\n",
        "dates = pd.date_range(start=start_date, end=end_date, periods=num_rows)\n",
        "\n",
        "# Assignation de la plage de dates à une nouvelle colonne 'ds'\n",
        "df_pandas['ds'] = dates\n",
        "fig = calplot(df_pandas,x=\"ds\",y=\"count\",title=\"Répartition des accidents pour chaque jour, des années 2021 et 2022\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "NHEHobKg4MgS"
      },
      "id": "NHEHobKg4MgS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Profil des usagers\n",
        "\n",
        "Ici, on défini d'abord quatre fonctions :\n",
        " * \"**df_to_dict**\" qui prend un dataframe et renvoie une liste de dictionnaires correspondant aux lignes du dataframe.\n",
        " * \"**catr_name**\" qui prend u dataframe et remplace les valeurs de la colonne \"*catr*\" par les le type de circulation, si c'est rurale ou urbaine.\n",
        " * \"**catr_name_bis**\" qui prend un dataframe et remplace les valeurs de la colonne \"*catr*\" par les noms correspondant.\n",
        " * \"**sexe_name**\" qui prend un dataframe et remplace les valeurs de la colonne \"*sexe*\" par le sexe de la personne.\n",
        " * \"**catu_name**\" qui prend un dataframe et remplace les valeurs de la colonne \"*catu*\" par la catégorie des usagers présent sur l'accident.\n",
        "\n",
        "On commence d'abord par lié le dataframe *usagers* à celui de *lieux*.\n",
        "On retire toutes les lignes où le sexe de la personne n'est pas renseignée, puis on regroupe notre dataframe sur les colonnes *catr* *catu* et *sexe* et on compte le nombres de lignes pour chaque groupe.\n",
        "\n",
        "On utilise la fonction **sexe_name**, **catu_name** et **catr_name_bis** sur le dataframe. On effectue une copie du dataframe pour pouvoir l'utiliser après. Enfin, on affiche un graphe en barre, sur deux lignes, une ligne avec les personnes de sexe féminin et l'autre masculin, avec en ordonnée le nombre d'usagers, et en abscisse la catégorie des routes. On met en couleur les différents catégorie d'usager : conducteur, piéton ou passager d'un véhicule.\n",
        "\n",
        "Ensuite, on fait un deuxième graphe à partir de notre copie : au lieu d'appeler **catr_name_bis**, on va appeler **catr_name** pour nous donner les types de circulation (urbaine ou rurale). On affiche des graphes circulaire, toujours sur deux lignes en fonction du sexe, en différenciant le type de circulation, et en fonction également de la catégorie des usagers.\n",
        "\n",
        "Finalement, on fini avec un troisième graphe qui représente le nombre d'usagers, par sexe, en fonction des années de naissances. On regroupe les usagers en fonction de leur sexe et de leur année de naissance, on compte le nombre de lignes, et on l'affiche dans ce graphe en barre.\n",
        "\n"
      ],
      "metadata": {
        "id": "jsfpn0Mb4MsZ"
      },
      "id": "jsfpn0Mb4MsZ"
    },
    {
      "cell_type": "code",
      "source": [
        " # Fonction qui créer une liste de dictionnaire correspondat aux lignes d'un dataframe\n",
        "def df_to_dict(df):\n",
        "  return [row.asDict() for row in df.collect()]\n",
        "\n",
        "# Fonction qui transforme les index catr en valeur\n",
        "def catr_name_bis(df):\n",
        "  return df.withColumn('catr',\n",
        "          F.when(df.catr == 1, 'Autoroute')\n",
        "          .when(df.catr == 2, 'Route nationale')\n",
        "          .when(df.catr == 3, 'Route Départementale')\n",
        "          .when(df.catr == 4, 'Voie Communales')\n",
        "          .when(df.catr == 5, 'Hors réseau public')\n",
        "          .when(df.catr == 6, 'Parc de stationnement ouvert à la circulation publique')\n",
        "          .when(df.catr == 7, 'Routes de métropole urbaine')\n",
        "          .when(df.catr == 9, 'Autre')\n",
        ")\n",
        "\n",
        "# Fonction qui transforme l'index des sexes en valeur\n",
        "def sexe_name(df):\n",
        "  return df.withColumn('sexe', F.when(df.sexe == 1, 'Male').when(df.sexe == 2, 'Female').otherwise('Unknown'))\n",
        "\n",
        "# Fonction qui transforme l'index des catu en valeur\n",
        "def catu_name(df):\n",
        "  return df.withColumn('catu', F.when(df.catu == 1, 'Conducteur')\n",
        "                              .when(df.catu == 2, 'Passager')\n",
        "                              .when(df.catu == 3, 'Piéton'))\n",
        "\n",
        "\n",
        "#On lie le dataframe users et locations\n",
        "df_circumstances = df_users.join(df_locations,'Num_Acc')\n",
        "\n",
        "\n",
        "# On filtre les lignes où le sexe égale à null\n",
        "df_circumstances = df_circumstances.filter(df_circumstances.sexe.isNotNull())\n",
        "\n",
        "# on veut le profil des usagers en fonction des circonstances.\n",
        "# On définit le profil des usagers par leur catégorie et de leur sexe.\n",
        "# On définit les circonstances en fonction de la catégorie de la route\n",
        "df_circumstances = df_circumstances.groupby(df_circumstances.catr, df_circumstances.catu, df_circumstances.sexe).count()\n",
        "\n",
        "\n",
        "# On remplace les valeurs indexées par ce qu'ils veulent dire pour que ça soit plus parlant\n",
        "df_circumstances = catu_name(df_circumstances)\n",
        "df_circumstances = sexe_name(df_circumstances)\n",
        "\n",
        "#On effectue une copie pour la suite, pour éviter de devoir refaire la même approche\n",
        "df_copie = df_circumstances\n",
        "\n",
        "#On remplace les valeurs indexées ici aussi\n",
        "df_circumstances = catr_name_bis(df_circumstances)\n",
        "\n",
        "fig = px.bar(df_to_dict(df_circumstances), title=\"Profils des usagers en fonction des catégories de routes\",\n",
        "             x='catr',\n",
        "             y='count',\n",
        "             color = 'catu',\n",
        "             facet_row = 'sexe',\n",
        "             barmode='group',\n",
        "             height=800\n",
        "             )\n",
        "\n",
        "fig.update_layout(xaxis={'categoryorder':'total descending'})\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "LkpO5JMS4NB5"
      },
      "id": "LkpO5JMS4NB5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction qui transforme l'index catr en valeur \"rurale\" ou \"urbaine\"\n",
        "def catr_name(df):\n",
        "  return df.withColumn('catr',\n",
        "          F.when(df.catr == 1, 'Circulation rurale')\n",
        "          .when(df.catr == 2, 'Circulation rurale')\n",
        "          .when(df.catr == 3, 'Circulation urbaine')\n",
        "          .when(df.catr == 4, 'Circulation urbaine')\n",
        "          .when(df.catr == 5, 'Circulation rurale')\n",
        "          .when(df.catr == 6, 'Circulation urbaine')\n",
        "          .when(df.catr == 7, 'Circulation urbaine')\n",
        "          .when(df.catr == 9, 'Circulation rurale')\n",
        ")\n",
        "\n",
        "# Pour distinguer les circonstances en fonction de la circulation urbaine ou en campagne\n",
        "df_copie = catr_name(df_copie)\n",
        "\n",
        "# Diagramme circulaire\n",
        "fig = px.pie(df_to_dict(df_copie), title=\"Profils des usagers en fonction de la circulation urbaine ou en campagne\",\n",
        "             names='catr',\n",
        "             values='count',\n",
        "             color = 'catr',\n",
        "             facet_row='sexe',\n",
        "             facet_col='catu',\n",
        "             height = 600\n",
        "             )\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "EDGeLn7Aypwv"
      },
      "id": "EDGeLn7Aypwv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On récupère le nombre de personne par date de naissance et par sexe\n",
        "birth_counts = df_users.groupby(df_users.an_nais, df_users.sexe).count()\n",
        "birth_counts = sexe_name(birth_counts)\n",
        "fig = px.bar(df_to_dict(birth_counts), title=\"Profils des usagers en fonction de l'année de naissance et de leur sexe\",\n",
        "             x='an_nais',\n",
        "             y='count',\n",
        "             color = 'sexe')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "RxEPRNOM0GTo"
      },
      "id": "RxEPRNOM0GTo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accidents impliquant des cyclistes et/ou des piétons\n",
        "\n",
        "On récupère d'abord le nombre de piétons par accident à l'aide du numéro de leurs catégorie, en n'oubliant pas de garder les colonnes utiles (celle qui ont au moins un piéton dans l'accident).\n",
        "Pour les cyclistes, on regarde la catégorie du véhicule, et de même, on compte le nombre de cycliste par accident.\n",
        "\n",
        "Pour les accidents impliquant des cyclistes ou des piétons, on fait l'union des deux dataframes récupéré juste avant, on compte le nombre de lignes (nombre d'accident) et on affiche le dataframe.\n",
        "\n",
        "Pour les accidents impliquant des cyclistes et des piétons, on fait la même chose simplement on procède en faisant l'intersection plutôt que l'union."
      ],
      "metadata": {
        "id": "U5Q4ir4A4NK0"
      },
      "id": "U5Q4ir4A4NK0"
    },
    {
      "cell_type": "code",
      "source": [
        "df_vehicle = merged_dfs[1]\n",
        "\n",
        "# On compte le nombre de piétons par accident\n",
        "df_pieton = df_users.where(df_users.catu == 3).distinct().groupby(df_users.Num_Acc).count()\n",
        "df_pieton = df_pieton.where(col('count') >= 1).select(\"Num_Acc\")\n",
        "\n",
        "\n",
        "df_bicycle = df_vehicle.where(df_vehicle.catv == 1).select(\"Num_Acc\").distinct()\n",
        "\n",
        "\n",
        "# Union des df\n",
        "df_union = df_pieton.union(df_bicycle)\n",
        "print(\"Accidents impliquant des cyclistes ou des piétons : \" , df_union.count())\n",
        "df_union.show()\n",
        "\n",
        "# Intersection\n",
        "df_intersect = df_pieton.intersect(df_bicycle)\n",
        "print(\"Accidents impliquant des cyclistes et des piétons : \" , df_intersect.count())\n",
        "df_intersect.show()\n"
      ],
      "metadata": {
        "id": "QL74aYMf4NRG"
      },
      "id": "QL74aYMf4NRG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Répartition des accidents sur la semaine (jours et heures)\n",
        "\n",
        "Pour la répartition, on procède la même manière que la première fois. On appelle la fonction **show_accidents_par_jour_et_heure**, créer plus tôt dans le fichier, mais cette fois sur les dataframes de l'union entre notre précédente union et le dataframe des caractéristiques, et l'union entre notre précédente intersection et le dataframe des caractéristiques.\n",
        "\n"
      ],
      "metadata": {
        "id": "M96mzTr1eG9z"
      },
      "id": "M96mzTr1eG9z"
    },
    {
      "cell_type": "code",
      "source": [
        "# Union de l'union piéton / cycliste avec le dataframe des caractéristiques (sur Num_Acc)\n",
        "df_union_cara = df_union.join(df_caracteristiques, 'Num_acc')\n",
        "# Union de l'intersection piéton / cycliste avec le dataframe des caractéristiques (sur Num_Acc)\n",
        "df_intersect_cara = df_intersect.join(df_caracteristiques, 'Num_acc')\n",
        "\n",
        "# On affiche de la même façon que tout à l'heure\n",
        "show_accidents_par_jour_et_heure(df_union_cara)\n",
        "show_accidents_par_jour_et_heure(df_intersect_cara)"
      ],
      "metadata": {
        "id": "p6zCvrFneTUc"
      },
      "id": "p6zCvrFneTUc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Répartition des accidents sur les mois de l’année\n",
        "\n",
        "Même principe que précédemment, toujours sur notre union d'union et caractéristique, ainsi que notre union d'intersection et caractéristiques, cette fois sur la fonction **show_accidents_par_mois**"
      ],
      "metadata": {
        "id": "ZvaXKLEWeURw"
      },
      "id": "ZvaXKLEWeURw"
    },
    {
      "cell_type": "code",
      "source": [
        "show_accidents_par_mois(df_union_cara)\n",
        "show_accidents_par_mois(df_intersect_cara)"
      ],
      "metadata": {
        "id": "YjBMx4aSeZah"
      },
      "id": "YjBMx4aSeZah",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Les caractéristiques des lieux où se sont produits ces accidents.\n",
        "\n",
        "Pour les caractéristiques des lieux des accidents, on commence d'abord par afficher une carte de densité, centré autour de la France métropolitaine. On affiche en couleur style \"heatmap\" lieux des accidents, avec la latitude et la longitude présente dans le dataframe *caracteristiques*\". Le problème de cette approche est qu'il est très rare qu'un accident arrive à exactement la même latitude et longitude d'un autre, souvent il y a un minuscule écart entre deux accidents, cependant en dézoomant correctement la carte, on peut quand même afficher de belles zones colorés où se sont passé des accidents.\n",
        "\n",
        "On défini trois fonctions **lum_name**, **age_name** et **int_name** qui remplacent les valeurs indexées de '*lum*', '*agg*' et '*int*' ne sont pas NULL.\n",
        "\n",
        "On regroupe le dataframe caracteristiques en fonction des colonnes '*lum*', '*agg*' et '*int*' et on compte le nombre d'occurences de chaque combinaison. On stocke ça dans un nouveau dataframe.\n",
        "On applique nos trois fonctions sur ce dataframe, et on créer un graphique en barres avec ce dataframe. Les barres sont regroupées par type d'intersection, avec une couleur différente pour chaque luminosité, et une ligne séparée si c'est en agglomération ou non."
      ],
      "metadata": {
        "id": "dcKYtoL95o0L"
      },
      "id": "dcKYtoL95o0L"
    },
    {
      "cell_type": "code",
      "source": [
        "df_pandas = df_caracteristiques.groupby(['lat', 'long']).count().toPandas()\n",
        "\n",
        "# Création de la carte de densité à l'aide de la latitude et longitude\n",
        "fig = px.density_mapbox(df_pandas, lat='lat', lon='long', z = 'count',\n",
        "                        radius=10,\n",
        "                        center=dict(lat=46.83, lon=2.5),\n",
        "                        zoom=4,\n",
        "                        mapbox_style='open-street-map',\n",
        "                        title='Les lieux où se sont produits les accidents',\n",
        "                        height = 600)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "HAGkqRIZGq14"
      },
      "id": "HAGkqRIZGq14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_cara_lieu = df_caracteristiques.groupby('lum', 'agg', 'int').count()\n",
        "\n",
        "# Fonction qui prend un dataframe et remplace les valeurs indexées de \"lum\"\n",
        "def lum_name(df):\n",
        "  return df.withColumn('lum',\n",
        "          F.when(df.lum == 1, 'Plein jour')\n",
        "          .when(df.lum == 2, 'Crépuscule ou aube')\n",
        "          .when(df.lum == 3, 'Nuit sans éclairage public')\n",
        "          .when(df.lum == 4, 'Nuit avec éclairage public non allumé')\n",
        "          .when(df.lum == 5, 'Nuit avec éclairage public allumé')\n",
        "  )\n",
        "\n",
        "# Fonction qui prend un dataframe et remplace les valeurs indexées de \"int\"\n",
        "def int_name(df):\n",
        "    return df.withColumn('int',\n",
        "          F.when(df.int == 1, 'Hors intersection')\n",
        "          .when(df.int == 2, 'Intersection en X')\n",
        "          .when(df.int == 3, 'Intersection en T')\n",
        "          .when(df.int == 4, 'Intersection en Y')\n",
        "          .when(df.int == 5, 'Intersection à plus de 4 branches')\n",
        "          .when(df.int == 6, 'Giratoire')\n",
        "          .when(df.int == 7, 'Place')\n",
        "          .when(df.int == 8, 'Passage à niveau')\n",
        "          .when(df.int == 9, 'Autre intersection')\n",
        "    )\n",
        "\n",
        "# Fonction qui prend un dataframe et remplace les valeurs indexées de \"agg\"\n",
        "def agg_name(df):\n",
        "  return df.withColumn('agg',\n",
        "          F.when(col('agg') == 1, 'Hors agglomération')\n",
        "          .when(col('agg') == 2, 'En agglomération')\n",
        "  )\n",
        "\n",
        "\n",
        "# On filtre toutes les valeurs non null de notre dataframe\n",
        "df_cara_lieu = df_cara_lieu.filter(col('int').isNotNull())\n",
        "df_cara_lieu = df_cara_lieu.filter(col('agg').isNotNull())\n",
        "df_cara_lieu = df_cara_lieu.filter(col('lum').isNotNull())\n",
        "\n",
        "# On change les index en valeurs\n",
        "df_cara_lieu = lum_name(df_cara_lieu)\n",
        "df_cara_lieu = agg_name(df_cara_lieu)\n",
        "df_cara_lieu = int_name(df_cara_lieu)\n",
        "\n",
        "# Graphe en barre\n",
        "fig = px.bar(df_to_dict(df_cara_lieu), title=\"Les caractéristiques (agglomération, luminosité, intersection) des lieux où se sont produits ces accidents.\",\n",
        "             x='int',\n",
        "             y='count',\n",
        "             color = 'lum',\n",
        "             facet_row = 'agg',\n",
        "             barmode='group',\n",
        "             height = 800)\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "kP4K7tgceIBB"
      },
      "id": "kP4K7tgceIBB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usage des types composites\n",
        "\n",
        "On crée une vue temporaire pour les dataframes *vehicule* et *lieux*, puis on éxécute une requête SQL pour joindre ces deux dataframes sur la colonne \"*Num_Acc*\". On récupère aussi l'id_vehicule et les voies des accidents.\n",
        "On crée un nouveau dataframe, basé sur un schéma comprenant le numéro de l'accident, le numéro du véhicule en cause et le lieu de l'accident. Enfin, on affiche ce dataframe."
      ],
      "metadata": {
        "id": "zhZYtYfG4NXX"
      },
      "id": "zhZYtYfG4NXX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Vue temporaire pour la query\n",
        "df_vehicle.createOrReplaceTempView(\"vehicle\")\n",
        "df_locations.createOrReplaceTempView(\"locations\")\n",
        "\n",
        "#Query\n",
        "query = \"\"\"\n",
        "    SELECT v.Num_acc, v.id_vehicule, l.voie\n",
        "    FROM vehicle as v\n",
        "    JOIN locations l ON l.Num_acc = v.Num_acc\n",
        "\"\"\"\n",
        "result = spark.sql(query)\n",
        "\n",
        "# Schema de notre structure de type pour les types composites\n",
        "schema = StructType([\n",
        "    StructField(\"Num_Acc\", LongType(), False),\n",
        "    StructField(\"Vehicule en cause\", IntegerType(), True),\n",
        "    StructField(\"Lieu\", StringType(), True)\n",
        "])\n",
        "# On créé notre dataframe a partir du schema précédent\n",
        "df_composite = spark.createDataFrame(result.rdd, schema=schema)\n",
        "\n",
        "df_composite.show()\n",
        "df_composite.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "id": "gLqsv_u66nVI"
      },
      "id": "gLqsv_u66nVI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sauvegarde au format parquet\n",
        "\n",
        "* On partitionne le dataframe \"usagers\" en fonction de la catégorie de l'usager (si c'est un piéton, passager ou conducteur). Cela nous semblait évident de partitionner de cette façon, cela permet de faire un petit partionnement, avec seulement trois possibilités, en fonction de type d'usagers auquel nous avons affaire.\n",
        "\n",
        "* Pour le dataframe \"lieux\", on partitionne en fonction de la catégorie de route. Les autres caractéristiques de lieux ne nous semblait pas intéressante pour une partition de parquet, alors que la catégorie de routes nous donne une vague idée sur quoi s'est dérouler un accident\n",
        "\n",
        "* Nous n'avons pas fait de partition pour le reste, car cela nous semblait sans interêt pour \"caractéristiques\" et \"vehicule\", il n'y avait pas de vrai valeur qui apportait un plus en terme de partition."
      ],
      "metadata": {
        "id": "7xqoknnS6ojY"
      },
      "id": "7xqoknnS6ojY"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for name, df in zip(names, dataframes):\n",
        "  if \"usagers\" in name: #Partition en fonction de la catégorie des usagers\n",
        "    df.write.mode('overwrite').partitionBy(\"catu\").parquet(\"data/\"+name+\"-cleaned.parquet\")\n",
        "  elif \"lieux\" in name: #Partition en fonction de la catégorie des lieux\n",
        "    df.write.mode('overwrite').partitionBy(\"catr\").parquet(\"data/\"+name+\"-cleaned.parquet\")\n",
        "  else : #Pas de partition particulière\n",
        "    df.write.mode('overwrite').parquet(\"data/\"+name+\"-cleaned.parquet\")\n"
      ],
      "metadata": {
        "id": "nRcvScqV6njP"
      },
      "id": "nRcvScqV6njP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls -al data/*cleaned.parquet"
      ],
      "metadata": {
        "id": "TZ3CS8YbXhzG"
      },
      "id": "TZ3CS8YbXhzG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "XNPzoSbfxG99"
      },
      "id": "XNPzoSbfxG99",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}